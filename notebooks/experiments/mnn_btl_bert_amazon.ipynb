{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.8.* in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 1)) (2.8.4)\n",
      "Requirement already satisfied: tensorflow-addons==0.18.0 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 2)) (0.18.0)\n",
      "Requirement already satisfied: wheel in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 3)) (0.40.0)\n",
      "Requirement already satisfied: pandas<2.0.0 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 5)) (1.2.2)\n",
      "Requirement already satisfied: scipy in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 6)) (1.10.1)\n",
      "Requirement already satisfied: matplotlib in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 7)) (3.7.1)\n",
      "Requirement already satisfied: tqdm in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 8)) (4.65.0)\n",
      "Requirement already satisfied: Pillow in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 9)) (9.5.0)\n",
      "Requirement already satisfied: kaggle in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from -r ../../requirements.txt (line 10)) (1.5.13)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (23.5.26)\n",
      "Requirement already satisfied: gast>=0.2.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (3.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (16.0.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (4.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (0.32.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.54.2)\n",
      "Requirement already satisfied: packaging in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorflow-addons==0.18.0->-r ../../requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorflow-addons==0.18.0->-r ../../requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from pandas<2.0.0->-r ../../requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from pandas<2.0.0->-r ../../requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from scikit-learn->-r ../../requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from scikit-learn->-r ../../requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from matplotlib->-r ../../requirements.txt (line 7)) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from matplotlib->-r ../../requirements.txt (line 7)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from matplotlib->-r ../../requirements.txt (line 7)) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from matplotlib->-r ../../requirements.txt (line 7)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from matplotlib->-r ../../requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from matplotlib->-r ../../requirements.txt (line 7)) (5.12.0)\n",
      "Requirement already satisfied: certifi in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from kaggle->-r ../../requirements.txt (line 10)) (2023.5.7)\n",
      "Requirement already satisfied: requests in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from kaggle->-r ../../requirements.txt (line 10)) (2.31.0)\n",
      "Requirement already satisfied: python-slugify in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from kaggle->-r ../../requirements.txt (line 10)) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from kaggle->-r ../../requirements.txt (line 10)) (1.26.16)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r ../../requirements.txt (line 7)) (3.15.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (2.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from requests->kaggle->-r ../../requirements.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from requests->kaggle->-r ../../requirements.txt (line 10)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from typeguard>=2.7->tensorflow-addons==0.18.0->-r ../../requirements.txt (line 2)) (6.6.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from python-slugify->kaggle->-r ../../requirements.txt (line 10)) (1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /pfs/data5/home/es/es_es/es_kamait02/miniconda3/envs/tf2.8/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /pfs/data5/home/es/es_es/es_kamait02/.local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.*->-r ../../requirements.txt (line 1)) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add directory above current directory to path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from utils.text_processing import CharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 18:00:40.378105: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 18:00:40.930498: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-06-14 18:00:40.930552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30967 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.get_memory_info(\"GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"./configs/mnn_btl_bert_amazon_lr_1e-05.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.abo import ABO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading abo-listings.tar...\n",
      "abo-listings.tar already exists.\n",
      "Downloading abo-images-small.tar...\n",
      "abo-images-small.tar already exists.\n",
      "Loading images...\n",
      "Loading texts...\n",
      "Importing listings CSV...\n",
      "Creating false samples/complement...\n",
      "Merging ground truth and complement...\n",
      "Concatenating attributes into description columns...\n",
      "Finishing up...\n",
      "Exporting to CSV...\n",
      "Data processing complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>path</th>\n",
       "      <th>product_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>back coverb0854lm94tamazon brand - solimo desi...</td>\n",
       "      <td>35/35f0a7ac.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mobile coverb072n7ggkbamazon brand - solimo de...</td>\n",
       "      <td>0f/0ff62b3e.jpg</td>\n",
       "      <td>FINENECKLACEBRACELETANKLET</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mobile coverb0854775hpamazon brand - solimo de...</td>\n",
       "      <td>62/62fa7e36.jpg</td>\n",
       "      <td>SHOES</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mobile coverb07tf1d7jgamazon brand - solimo de...</td>\n",
       "      <td>c0/c0bfa0d3.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mobile coverb07tcwrmj7amazon brand - solimo de...</td>\n",
       "      <td>a6/a6aa41af.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168202</th>\n",
       "      <td>mobile coverb081hmphvtamazon brand - solimo de...</td>\n",
       "      <td>ea/eaeeefdd.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168203</th>\n",
       "      <td>back coverb07fnbjw24amazon brand - solimo desi...</td>\n",
       "      <td>db/dbb1be1e.jpg</td>\n",
       "      <td>PORTABLE_ELECTRONIC_DEVICE_COVER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168204</th>\n",
       "      <td>mobile coverb07th3ccxmamazon brand - solimo de...</td>\n",
       "      <td>87/87f570ad.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168205</th>\n",
       "      <td>formal shoes for mens leatherb07tg4wgvbamazon ...</td>\n",
       "      <td>4c/4cfdeb1f.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168206</th>\n",
       "      <td>organic quinoa grainb07tcwt1fgamazon brand - s...</td>\n",
       "      <td>42/42031cf0.jpg</td>\n",
       "      <td>PILLOW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168207 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description             path  \\\n",
       "0       back coverb0854lm94tamazon brand - solimo desi...  35/35f0a7ac.jpg   \n",
       "1       mobile coverb072n7ggkbamazon brand - solimo de...  0f/0ff62b3e.jpg   \n",
       "2       mobile coverb0854775hpamazon brand - solimo de...  62/62fa7e36.jpg   \n",
       "3       mobile coverb07tf1d7jgamazon brand - solimo de...  c0/c0bfa0d3.jpg   \n",
       "4       mobile coverb07tcwrmj7amazon brand - solimo de...  a6/a6aa41af.jpg   \n",
       "...                                                   ...              ...   \n",
       "168202  mobile coverb081hmphvtamazon brand - solimo de...  ea/eaeeefdd.jpg   \n",
       "168203  back coverb07fnbjw24amazon brand - solimo desi...  db/dbb1be1e.jpg   \n",
       "168204  mobile coverb07th3ccxmamazon brand - solimo de...  87/87f570ad.jpg   \n",
       "168205  formal shoes for mens leatherb07tg4wgvbamazon ...  4c/4cfdeb1f.jpg   \n",
       "168206  organic quinoa grainb07tcwt1fgamazon brand - s...  42/42031cf0.jpg   \n",
       "\n",
       "                            product_type  label  \n",
       "0                    CELLULAR_PHONE_CASE      1  \n",
       "1             FINENECKLACEBRACELETANKLET      0  \n",
       "2                                  SHOES      0  \n",
       "3                    CELLULAR_PHONE_CASE      1  \n",
       "4                    CELLULAR_PHONE_CASE      0  \n",
       "...                                  ...    ...  \n",
       "168202               CELLULAR_PHONE_CASE      1  \n",
       "168203  PORTABLE_ELECTRONIC_DEVICE_COVER      0  \n",
       "168204               CELLULAR_PHONE_CASE      1  \n",
       "168205               CELLULAR_PHONE_CASE      0  \n",
       "168206                            PILLOW      0  \n",
       "\n",
       "[168207 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ABO(path=config[\"data\"][\"path\"],\n",
    "           download=True,\n",
    "           extract=False,\n",
    "           preprocess=True,\n",
    "           alt_augment=False,\n",
    "           random_deletion=False,\n",
    "           export_csv=True).data\n",
    "\n",
    "# data = pd.read_csv(os.path.join(config[\"data\"][\"path\"], \"data.csv\"))\n",
    "# data = data.drop({\"Unnamed: 0\"}, axis=1)\n",
    "\n",
    "data = data[['description', 'path', 'product_type', 'label']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data[[\"path\"]]  # two brackets for keeping the column name\n",
    "text = data[\"description\"]\n",
    "product_types = data[[\"product_type\"]]\n",
    "labels = data[[\"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is largely based off of https://www.tensorflow.org/text/guide/bert_preprocessing_guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate text embeddings directly with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = bert_preprocess_model(text)\n",
    "\n",
    "# print(f'Keys       : {list(text.keys())}')\n",
    "# print(f'Shape      : {text[\"input_word_ids\"].shape}')\n",
    "# print(f'Word Ids   : {text[\"input_word_ids\"][0, :12]}')\n",
    "# print(f'Input Mask : {text[\"input_mask\"][0, :12]}')\n",
    "# print(f'Type Ids   : {text[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = bert_model(text)\n",
    "\n",
    "# print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "# print(f'Pooled Outputs Shape:{text[\"pooled_output\"].shape}')\n",
    "# print(f'Pooled Outputs Values:{text[\"pooled_output\"][0, :12]}')\n",
    "# print(f'Sequence Outputs Shape:{text[\"sequence_output\"].shape}')\n",
    "# print(f'Sequence Outputs Values:{text[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text[\"pooled_output\"]\n",
    "# text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.img_processing import load_img_model, create_embeddings_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 576)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_model = load_img_model(config[\"img_model\"])\n",
    "\n",
    "img_model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168171 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# Mobilenetv3small:\n",
    "# batch_size = 1024 -> 8 GB VRAM\n",
    "# batch_size = 2048 -> 16 GB VRAM\n",
    "# ...\n",
    "# Mobilenetv3large: twice as much as Mobilenetv3small\n",
    "\n",
    "img = create_embeddings_from(img_model,\n",
    "                             img,\n",
    "                             os.path.join(config[\"data\"][\"path\"],\n",
    "                                          \"images/small\"),\n",
    "                             batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168171, 576)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(config[\"data\"][\"path\"],\n",
    "                         f\"embeddings/bert/{img_model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_path, exist_ok=True)\n",
    "np.save(f\"{save_path}/img.npy\", img)\n",
    "np.save(f\"{save_path}/text.npy\", text)\n",
    "data.to_csv(f\"{save_path}/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>description2</th>\n",
       "      <th>path</th>\n",
       "      <th>product_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shoesb0791vjhksamazon brand - symbol men's sne...</td>\n",
       "      <td>shoesb0791vjhksamazon brand - symbol men's sne...</td>\n",
       "      <td>4d/4d552519.jpg</td>\n",
       "      <td>SHOES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mobile coverb07tg4cyjvamazon brand - solimo de...</td>\n",
       "      <td>mobile coverb07tg4cyjvamazon brand - solimo de...</td>\n",
       "      <td>4d/4de76f7f.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mobile coverb0856b8fw1sunshine swatch, ravenna...</td>\n",
       "      <td>combo cooker dutch oven for bread making ovens...</td>\n",
       "      <td>72/72c5b6c9.jpg</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole foods, whole food, whole foods,365 every...</td>\n",
       "      <td>cellphonecoverb07tf7gr3ywhole foods market cre...</td>\n",
       "      <td>85/854b1468.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>back coverb00no73q84amazon brand - solimo desi...</td>\n",
       "      <td>mobile coverb071gvmr85amazon brand - solimo de...</td>\n",
       "      <td>6a/6a127d3e.jpg</td>\n",
       "      <td>BOOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168221</th>\n",
       "      <td>mobile coverb07y33f9kxamazon brand - solimo de...</td>\n",
       "      <td>handbags for women latest brandedb072kwqy8yama...</td>\n",
       "      <td>e6/e6ed7731.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168222</th>\n",
       "      <td>amazonbasics;amzn-lt800p-a;replacement-refrige...</td>\n",
       "      <td>amazonbasics;amzn-lt800p-a;replacement-refrige...</td>\n",
       "      <td>73/73903c65.jpg</td>\n",
       "      <td>MAJOR_HOME_APPLIANCES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168223</th>\n",
       "      <td>placeholderb074h67h9qamazon brand - solimo des...</td>\n",
       "      <td>deb0854kjcylamazon brand - solimo designer cou...</td>\n",
       "      <td>10/10b2c3af.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168224</th>\n",
       "      <td>mobile coverb0854ljhw2half tofu banh mi</td>\n",
       "      <td>mobile coverb081hmxdkvfind. women's velvet str...</td>\n",
       "      <td>de/de19c121.jpg</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168225</th>\n",
       "      <td>mobile coverb08gkxpbbzamazon brand - vedaka pr...</td>\n",
       "      <td>mobile coverb08569y2r3amazon brand - solimo de...</td>\n",
       "      <td>6d/6d147b0c.jpg</td>\n",
       "      <td>WALLET</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168226 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description  \\\n",
       "0       shoesb0791vjhksamazon brand - symbol men's sne...   \n",
       "1       mobile coverb07tg4cyjvamazon brand - solimo de...   \n",
       "2       mobile coverb0856b8fw1sunshine swatch, ravenna...   \n",
       "3       whole foods, whole food, whole foods,365 every...   \n",
       "4       back coverb00no73q84amazon brand - solimo desi...   \n",
       "...                                                   ...   \n",
       "168221  mobile coverb07y33f9kxamazon brand - solimo de...   \n",
       "168222  amazonbasics;amzn-lt800p-a;replacement-refrige...   \n",
       "168223  placeholderb074h67h9qamazon brand - solimo des...   \n",
       "168224            mobile coverb0854ljhw2half tofu banh mi   \n",
       "168225  mobile coverb08gkxpbbzamazon brand - vedaka pr...   \n",
       "\n",
       "                                             description2             path  \\\n",
       "0       shoesb0791vjhksamazon brand - symbol men's sne...  4d/4d552519.jpg   \n",
       "1       mobile coverb07tg4cyjvamazon brand - solimo de...  4d/4de76f7f.jpg   \n",
       "2       combo cooker dutch oven for bread making ovens...  72/72c5b6c9.jpg   \n",
       "3       cellphonecoverb07tf7gr3ywhole foods market cre...  85/854b1468.jpg   \n",
       "4       mobile coverb071gvmr85amazon brand - solimo de...  6a/6a127d3e.jpg   \n",
       "...                                                   ...              ...   \n",
       "168221  handbags for women latest brandedb072kwqy8yama...  e6/e6ed7731.jpg   \n",
       "168222  amazonbasics;amzn-lt800p-a;replacement-refrige...  73/73903c65.jpg   \n",
       "168223  deb0854kjcylamazon brand - solimo designer cou...  10/10b2c3af.jpg   \n",
       "168224  mobile coverb081hmxdkvfind. women's velvet str...  de/de19c121.jpg   \n",
       "168225  mobile coverb08569y2r3amazon brand - solimo de...  6d/6d147b0c.jpg   \n",
       "\n",
       "                 product_type  label  \n",
       "0                       SHOES      1  \n",
       "1         CELLULAR_PHONE_CASE      1  \n",
       "2                     GROCERY      0  \n",
       "3         CELLULAR_PHONE_CASE      0  \n",
       "4                        BOOT      0  \n",
       "...                       ...    ...  \n",
       "168221    CELLULAR_PHONE_CASE      0  \n",
       "168222  MAJOR_HOME_APPLIANCES      1  \n",
       "168223    CELLULAR_PHONE_CASE      0  \n",
       "168224                GROCERY      0  \n",
       "168225                 WALLET      0  \n",
       "\n",
       "[168226 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(f\"{save_path}/data.csv\")\n",
    "data = data.drop({\"Unnamed: 0\"}, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168226, 576)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.load(f\"{save_path}/img.npy\", allow_pickle=True)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168226,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = np.load(f\"{save_path}/text.npy\", allow_pickle=True)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into ground truth/false samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `stratify` parameter of `sklearn.model_selection.train_test_split()`, we need to select all product instances which appear more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>description2</th>\n",
       "      <th>path</th>\n",
       "      <th>product_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shoesb0791vjhksamazon brand - symbol men's sne...</td>\n",
       "      <td>shoesb0791vjhksamazon brand - symbol men's sne...</td>\n",
       "      <td>4d/4d552519.jpg</td>\n",
       "      <td>SHOES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mobile coverb07tg4cyjvamazon brand - solimo de...</td>\n",
       "      <td>mobile coverb07tg4cyjvamazon brand - solimo de...</td>\n",
       "      <td>4d/4de76f7f.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>samsung m31 360 coverb08563mt9famazon brand - ...</td>\n",
       "      <td>samsung m31 360 coverb08563mt9famazon brand - ...</td>\n",
       "      <td>cc/cc1f9aef.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mobile coverb07tg44qnnamazon brand - solimo de...</td>\n",
       "      <td>mobile coverb07tg44qnnamazon brand - solimo de...</td>\n",
       "      <td>6b/6be7b7d0.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mobile coverb07t22ks83amazon brand - solimo de...</td>\n",
       "      <td>mobile coverb07t22ks83amazon brand - solimo de...</td>\n",
       "      <td>04/04287092.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168216</th>\n",
       "      <td>back coverb0854ctp5namazon brand - solimo desi...</td>\n",
       "      <td>back coverb0854ctp5namazon brand - solimo desi...</td>\n",
       "      <td>03/03f0cbf6.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168217</th>\n",
       "      <td>mobile coverb081hnqrqjamazon brand - solimo de...</td>\n",
       "      <td>mobile coverb081hnqrqjamazon brand - solimo de...</td>\n",
       "      <td>67/67d2b3f8.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168219</th>\n",
       "      <td>shoes for women stylishb07xrx83cmamazon brand ...</td>\n",
       "      <td>shoes for women stylishb07xrx83cmamazon brand ...</td>\n",
       "      <td>d7/d77cc014.jpg</td>\n",
       "      <td>SHOES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168220</th>\n",
       "      <td>arena strength pilates band resistance bands s...</td>\n",
       "      <td>arena strength pilates band resistance bands s...</td>\n",
       "      <td>d3/d3e492c8.jpg</td>\n",
       "      <td>EXERCISE_BAND</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168222</th>\n",
       "      <td>amazonbasics;amzn-lt800p-a;replacement-refrige...</td>\n",
       "      <td>amazonbasics;amzn-lt800p-a;replacement-refrige...</td>\n",
       "      <td>73/73903c65.jpg</td>\n",
       "      <td>MAJOR_HOME_APPLIANCES</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87380 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description  \\\n",
       "0       shoesb0791vjhksamazon brand - symbol men's sne...   \n",
       "1       mobile coverb07tg4cyjvamazon brand - solimo de...   \n",
       "6       samsung m31 360 coverb08563mt9famazon brand - ...   \n",
       "8       mobile coverb07tg44qnnamazon brand - solimo de...   \n",
       "12      mobile coverb07t22ks83amazon brand - solimo de...   \n",
       "...                                                   ...   \n",
       "168216  back coverb0854ctp5namazon brand - solimo desi...   \n",
       "168217  mobile coverb081hnqrqjamazon brand - solimo de...   \n",
       "168219  shoes for women stylishb07xrx83cmamazon brand ...   \n",
       "168220  arena strength pilates band resistance bands s...   \n",
       "168222  amazonbasics;amzn-lt800p-a;replacement-refrige...   \n",
       "\n",
       "                                             description2             path  \\\n",
       "0       shoesb0791vjhksamazon brand - symbol men's sne...  4d/4d552519.jpg   \n",
       "1       mobile coverb07tg4cyjvamazon brand - solimo de...  4d/4de76f7f.jpg   \n",
       "6       samsung m31 360 coverb08563mt9famazon brand - ...  cc/cc1f9aef.jpg   \n",
       "8       mobile coverb07tg44qnnamazon brand - solimo de...  6b/6be7b7d0.jpg   \n",
       "12      mobile coverb07t22ks83amazon brand - solimo de...  04/04287092.jpg   \n",
       "...                                                   ...              ...   \n",
       "168216  back coverb0854ctp5namazon brand - solimo desi...  03/03f0cbf6.jpg   \n",
       "168217  mobile coverb081hnqrqjamazon brand - solimo de...  67/67d2b3f8.jpg   \n",
       "168219  shoes for women stylishb07xrx83cmamazon brand ...  d7/d77cc014.jpg   \n",
       "168220  arena strength pilates band resistance bands s...  d3/d3e492c8.jpg   \n",
       "168222  amazonbasics;amzn-lt800p-a;replacement-refrige...  73/73903c65.jpg   \n",
       "\n",
       "                 product_type  label  \n",
       "0                       SHOES      1  \n",
       "1         CELLULAR_PHONE_CASE      1  \n",
       "6         CELLULAR_PHONE_CASE      1  \n",
       "8         CELLULAR_PHONE_CASE      1  \n",
       "12        CELLULAR_PHONE_CASE      1  \n",
       "...                       ...    ...  \n",
       "168216    CELLULAR_PHONE_CASE      1  \n",
       "168217    CELLULAR_PHONE_CASE      1  \n",
       "168219                  SHOES      1  \n",
       "168220          EXERCISE_BAND      1  \n",
       "168222  MAJOR_HOME_APPLIANCES      1  \n",
       "\n",
       "[87380 rows x 5 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = data[data[\"label\"] == 1]\n",
    "false_samples = data[data[\"label\"] == 0]\n",
    "data = ground_truth\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>description2</th>\n",
       "      <th>path</th>\n",
       "      <th>product_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mobile coverb0856b8fw1sunshine swatch, ravenna...</td>\n",
       "      <td>combo cooker dutch oven for bread making ovens...</td>\n",
       "      <td>72/72c5b6c9.jpg</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole foods, whole food, whole foods,365 every...</td>\n",
       "      <td>cellphonecoverb07tf7gr3ywhole foods market cre...</td>\n",
       "      <td>85/854b1468.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>back coverb00no73q84amazon brand - solimo desi...</td>\n",
       "      <td>mobile coverb071gvmr85amazon brand - solimo de...</td>\n",
       "      <td>6a/6a127d3e.jpg</td>\n",
       "      <td>BOOT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>back coverb07t9tlvvzamazonbasics youth basebal...</td>\n",
       "      <td>mobile coverb0856b4tqqamazon brand - solimo de...</td>\n",
       "      <td>ec/ec078182.jpg</td>\n",
       "      <td>HOME</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mobile coverb07tg4m9mbamazon brand -rivet midt...</td>\n",
       "      <td>cellphonecoverb07tg3nw24amazon brand - solimo ...</td>\n",
       "      <td>e7/e740caed.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168218</th>\n",
       "      <td>classics with a twistb07z43sk54amazon brand - ...</td>\n",
       "      <td>pancake syrupb0857kv499amazon brand - solimo d...</td>\n",
       "      <td>05/05fa30f2.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168221</th>\n",
       "      <td>mobile coverb07y33f9kxamazon brand - solimo de...</td>\n",
       "      <td>handbags for women latest brandedb072kwqy8yama...</td>\n",
       "      <td>e6/e6ed7731.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168223</th>\n",
       "      <td>placeholderb074h67h9qamazon brand - solimo des...</td>\n",
       "      <td>deb0854kjcylamazon brand - solimo designer cou...</td>\n",
       "      <td>10/10b2c3af.jpg</td>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168224</th>\n",
       "      <td>mobile coverb0854ljhw2half tofu banh mi</td>\n",
       "      <td>mobile coverb081hmxdkvfind. women's velvet str...</td>\n",
       "      <td>de/de19c121.jpg</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168225</th>\n",
       "      <td>mobile coverb08gkxpbbzamazon brand - vedaka pr...</td>\n",
       "      <td>mobile coverb08569y2r3amazon brand - solimo de...</td>\n",
       "      <td>6d/6d147b0c.jpg</td>\n",
       "      <td>WALLET</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80846 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description  \\\n",
       "2       mobile coverb0856b8fw1sunshine swatch, ravenna...   \n",
       "3       whole foods, whole food, whole foods,365 every...   \n",
       "4       back coverb00no73q84amazon brand - solimo desi...   \n",
       "5       back coverb07t9tlvvzamazonbasics youth basebal...   \n",
       "7       mobile coverb07tg4m9mbamazon brand -rivet midt...   \n",
       "...                                                   ...   \n",
       "168218  classics with a twistb07z43sk54amazon brand - ...   \n",
       "168221  mobile coverb07y33f9kxamazon brand - solimo de...   \n",
       "168223  placeholderb074h67h9qamazon brand - solimo des...   \n",
       "168224            mobile coverb0854ljhw2half tofu banh mi   \n",
       "168225  mobile coverb08gkxpbbzamazon brand - vedaka pr...   \n",
       "\n",
       "                                             description2             path  \\\n",
       "2       combo cooker dutch oven for bread making ovens...  72/72c5b6c9.jpg   \n",
       "3       cellphonecoverb07tf7gr3ywhole foods market cre...  85/854b1468.jpg   \n",
       "4       mobile coverb071gvmr85amazon brand - solimo de...  6a/6a127d3e.jpg   \n",
       "5       mobile coverb0856b4tqqamazon brand - solimo de...  ec/ec078182.jpg   \n",
       "7       cellphonecoverb07tg3nw24amazon brand - solimo ...  e7/e740caed.jpg   \n",
       "...                                                   ...              ...   \n",
       "168218  pancake syrupb0857kv499amazon brand - solimo d...  05/05fa30f2.jpg   \n",
       "168221  handbags for women latest brandedb072kwqy8yama...  e6/e6ed7731.jpg   \n",
       "168223  deb0854kjcylamazon brand - solimo designer cou...  10/10b2c3af.jpg   \n",
       "168224  mobile coverb081hmxdkvfind. women's velvet str...  de/de19c121.jpg   \n",
       "168225  mobile coverb08569y2r3amazon brand - solimo de...  6d/6d147b0c.jpg   \n",
       "\n",
       "               product_type  label  \n",
       "2                   GROCERY      0  \n",
       "3       CELLULAR_PHONE_CASE      0  \n",
       "4                      BOOT      0  \n",
       "5                      HOME      0  \n",
       "7       CELLULAR_PHONE_CASE      0  \n",
       "...                     ...    ...  \n",
       "168218  CELLULAR_PHONE_CASE      0  \n",
       "168221  CELLULAR_PHONE_CASE      0  \n",
       "168223  CELLULAR_PHONE_CASE      0  \n",
       "168224              GROCERY      0  \n",
       "168225               WALLET      0  \n",
       "\n",
       "[80846 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_false = img[false_samples.index]\n",
    "text_false = text[false_samples.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img[data.index]\n",
    "text = text[data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "false_samples = false_samples.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87380, 576)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80846, 576)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_false.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"product_type_count\"] = data.groupby(\n",
    "    [\"product_type\"])[\"product_type\"].transform(\"count\")\n",
    "\n",
    "data = data[data[\"product_type_count\"] > config[\"data\"][\"cls\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update both columns\n",
    "product_types = data[[\"product_type\"]]\n",
    "\n",
    "labels = data[[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, product_types_train, product_types_test = train_test_split(\n",
    "    data,\n",
    "    product_types,\n",
    "    stratify=product_types,\n",
    "    test_size=config[\"model\"][\"training\"][\"test_split\"],\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img[train.index]\n",
    "img_test = img[test.index]\n",
    "\n",
    "text_train = text[train.index]\n",
    "text_test = text[test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_samples[\"product_type_count\"] = false_samples.groupby(\n",
    "    [\"product_type\"])[\"product_type\"].transform(\"count\")\n",
    "\n",
    "false_samples = false_samples[false_samples[\"product_type_count\"] > config[\"data\"][\"cls\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update both columns\n",
    "product_types_false = false_samples[[\"product_type\"]]\n",
    "\n",
    "labels_false = false_samples[[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_false, test_false, product_types_train_false, product_types_test_false = train_test_split(\n",
    "    false_samples,\n",
    "    product_types_false,\n",
    "    stratify=product_types_false,\n",
    "    test_size=config[\"model\"][\"training\"][\"test_split\"],\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train_false = img_false[train_false.index]\n",
    "img_test_false = img_false[test_false.index]\n",
    "\n",
    "text_train_false = text_false[train_false.index]\n",
    "text_test_false = text_false[test_false.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build MNN-BTL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import create_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_config = {\n",
    "    \"img_input_size\": img_model.layers[-1].output_shape[1],\n",
    "    \"txt_input_size\": config[\"data\"][\"input_size\"],\n",
    "    \"img_fc_layers\": config[\"model\"][\"img_fc_layers\"],\n",
    "    \"txt_fc_layers\": config[\"model\"][\"txt_fc_layers\"],\n",
    "    \"extended\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Concatenate, Embedding, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from models.mnn_em import _CNNBranch\n",
    "from models.addons.tensorflow_addons.losses import triplet_multimodal\n",
    "\n",
    "\n",
    "class MNNBTLBert(object):\n",
    "    def __init__(self, head_config, learning_rate, name=\"MNN_BTL\"):\n",
    "        self.head_config = head_config\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        self._build_model()  # builds self.model variable\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Text Input\n",
    "        img_features = Input(shape=(self.head_config[\"img_input_size\"]),\n",
    "                             name=\"Image_Input_Head_Outer\")\n",
    "\n",
    "        img_cnn = _CNNBranch(self.head_config[\"img_input_size\"],\n",
    "                             self.head_config[\"img_fc_layers\"], self.head_config[\"extended\"], True, name=\"Image\")\n",
    "\n",
    "        # Image Input\n",
    "        text_features = tf.keras.layers.Input(shape=(()), dtype=tf.string, name=\"Text_Input_Head_Outer\")\n",
    "\n",
    "        bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "        bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "\n",
    "        x = bert_preprocess_model(text_features)\n",
    "\n",
    "        x = bert_model(x)[\"pooled_output\"]\n",
    "\n",
    "        text_branch = _CNNBranch(x.shape[1],\n",
    "                                 self.head_config[\"txt_fc_layers\"], self.head_config[\"extended\"], True, name=\"Text\")\n",
    "\n",
    "        output_text_branch = text_branch.model(x)\n",
    "\n",
    "        text_cnn = Model(inputs=text_features,\n",
    "                         outputs=output_text_branch, name=\"Text_CNN\")\n",
    "\n",
    "        model = Model(inputs=[img_features, text_features],\n",
    "                      outputs=Concatenate()(\n",
    "                          [img_cnn.model(img_features), text_cnn(text_features)]),\n",
    "                      name=self.name)\n",
    "\n",
    "        optimizer = Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        loss = triplet_multimodal.MultimodalTripletHardLossBidirectional(\n",
    "            distance_metric=\"angular\")\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss)\n",
    "\n",
    "        self.model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnn_btl = MNNBTLBert(\n",
    "    head_config=head_config,\n",
    "    learning_rate=config[\"model\"][\"training\"][\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MNN_BTL\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Image_Input_Head_Outer (InputL  [(None, 576)]       0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " Text_Input_Head_Outer (InputLa  [(None,)]           0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " Image_CNN (Functional)         (None, 512)          1115648     ['Image_Input_Head_Outer[0][0]'] \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| Image_Input_Head_Inner (InputL  [(None, 576)]     0           []                               |\n",
      "| ayer)                                                                                          |\n",
      "|                                                                                                |\n",
      "| Image_FC_1 (Dense)           (None, 1024)         590848      []                               |\n",
      "|                                                                                                |\n",
      "| Image_FC_last (Dense)        (None, 512)          524800      []                               |\n",
      "|                                                                                                |\n",
      "| Image_L2_Norm (Lambda)       (None, 512)          0           []                               |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " Text_CNN (Functional)          (None, 512)          110794497   ['Text_Input_Head_Outer[0][0]']  \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| Text_Input_Head_Outer (InputLa  [(None,)]         0           []                               |\n",
      "| yer)                                                                                           |\n",
      "|                                                                                                |\n",
      "| keras_layer_6 (KerasLayer)   {'input_type_ids':   0           []                               |\n",
      "|                              (None, 128),                                                      |\n",
      "|                               'input_word_ids':                                                |\n",
      "|                              (None, 128),                                                      |\n",
      "|                               'input_mask': (Non                                               |\n",
      "|                              e, 128)}                                                          |\n",
      "|                                                                                                |\n",
      "| keras_layer_7 (KerasLayer)   {'pooled_output': (  109482241   []                               |\n",
      "|                              None, 768),                                                       |\n",
      "|                               'sequence_output':                                               |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               'encoder_outputs':                                               |\n",
      "|                               [(None, 128, 768),                                               |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768),                                                |\n",
      "|                               (None, 128, 768)],                                               |\n",
      "|                               'default': (None,                                                |\n",
      "|                              768)}                                                             |\n",
      "|                                                                                                |\n",
      "| Text_CNN (Functional)        (None, 512)          1312256     []                               |\n",
      "||¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯||\n",
      "|| Text_Input_Head_Inner (InputLa  [(None, 768)]   0           []                               ||\n",
      "|| yer)                                                                                         ||\n",
      "||                                                                                              ||\n",
      "|| Text_FC_1 (Dense)          (None, 1024)         787456      []                               ||\n",
      "||                                                                                              ||\n",
      "|| Text_FC_last (Dense)       (None, 512)          524800      []                               ||\n",
      "||                                                                                              ||\n",
      "|| Text_L2_Norm (Lambda)      (None, 512)          0           []                               ||\n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " concatenate (Concatenate)      (None, 1024)         0           ['Image_CNN[0][0]',              \n",
      "                                                                  'Text_CNN[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,910,145\n",
      "Trainable params: 2,427,904\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnn_btl.model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# \"Head\"\n",
    "\n",
    "tf.keras.utils.plot_model(mnn_btl.model,\n",
    "                          rankdir=\"TB\",\n",
    "                          show_layer_activations=True,\n",
    "                          show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import create_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = config[\"model\"][\"training\"][\"log_dir\"]\n",
    "model_name = config[\"model\"][\"name\"]\n",
    "optimizer_name = config[\"model\"][\"training\"][\"optimizer\"]\n",
    "learning_rate = config[\"model\"][\"training\"][\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = create_callbacks(\n",
    "    callbacks_list=config[\"model\"][\"training\"][\"callbacks\"],\n",
    "    log_dir=log_dir,\n",
    "    model_name=model_name,\n",
    "    img_model_name=img_model.name,\n",
    "    optimizer_name=optimizer_name,\n",
    "    learning_rate=learning_rate,\n",
    "    cls=config[\"data\"][\"cls\"],\n",
    "    patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=product_types_train[\"product_type\"].unique(),\n",
    "                                                 y=product_types_train[\"product_type\"])\n",
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights_dict = {}\n",
    "class_weights_dict_transform = {}\n",
    "i = 0\n",
    "\n",
    "for pt, cw in zip(product_types_train[\"product_type\"].unique(), class_weights):\n",
    "    class_weights_dict[i] = cw\n",
    "    class_weights_dict_transform[pt] = i\n",
    "    i += 1\n",
    "    \n",
    "len(class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_weights_dict_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_type_transform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35850</th>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63788</th>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18615</th>\n",
       "      <td>RUG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51760</th>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32363</th>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63997</th>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34287</th>\n",
       "      <td>CELLULAR_PHONE_CASE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46446</th>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50996</th>\n",
       "      <td>SCREEN_PROTECTOR</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71545</th>\n",
       "      <td>KITCHEN</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78562 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              product_type  product_type_transform\n",
       "35850  CELLULAR_PHONE_CASE                       0\n",
       "63788  CELLULAR_PHONE_CASE                       0\n",
       "18615                  RUG                       1\n",
       "51760  CELLULAR_PHONE_CASE                       0\n",
       "32363  CELLULAR_PHONE_CASE                       0\n",
       "...                    ...                     ...\n",
       "63997  CELLULAR_PHONE_CASE                       0\n",
       "34287  CELLULAR_PHONE_CASE                       0\n",
       "46446               BEAUTY                      40\n",
       "50996     SCREEN_PROTECTOR                      46\n",
       "71545              KITCHEN                      48\n",
       "\n",
       "[78562 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_types_train[\"product_type_transform\"] = product_types_train[\"product_type\"].apply(lambda x: class_weights_dict_transform[x])\n",
    "product_types_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003965354505579923"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 5.4088"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 18:07:50.245405: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 302s 66ms/step - loss: 5.4087 - val_loss: 1.0781\n",
      "Epoch 2/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.2986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 288s 65ms/step - loss: 0.2986 - val_loss: 0.0509\n",
      "Epoch 3/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.0359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 291s 66ms/step - loss: 0.0359 - val_loss: 0.0294\n",
      "Epoch 4/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.0266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 289s 65ms/step - loss: 0.0266 - val_loss: 0.0268\n",
      "Epoch 5/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.0249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 289s 65ms/step - loss: 0.0249 - val_loss: 0.0251\n",
      "Epoch 6/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.0238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 291s 66ms/step - loss: 0.0238 - val_loss: 0.0243\n",
      "Epoch 7/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.0238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 289s 65ms/step - loss: 0.0238 - val_loss: 0.0241\n",
      "Epoch 8/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.0232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 291s 66ms/step - loss: 0.0232 - val_loss: 0.0238\n",
      "Epoch 9/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.0229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 290s 66ms/step - loss: 0.0229 - val_loss: 0.0236\n",
      "Epoch 10/10\n",
      "4419/4420 [============================>.] - ETA: 0s - loss: 0.0228"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Image_Input_Head_Outer, Text_Input_Head_Outer with unsupported characters which will be renamed to image_input_head_outer, text_input_head_outer in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./runs/models/MNN_BTL_BERT_ABO/cls_1/MobilenetV3small/Adam/lr_1e-05/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420/4420 [==============================] - 290s 66ms/step - loss: 0.0228 - val_loss: 0.0236\n"
     ]
    }
   ],
   "source": [
    "history = mnn_btl.model.fit(\n",
    "    x=[img_train, text_train],\n",
    "    y=product_types_train[\"product_type_transform\"],\n",
    "    epochs=config[\"model\"][\"training\"][\"epochs\"],\n",
    "    validation_split=config[\"model\"][\"training\"][\"validation_split\"],\n",
    "    batch_size=config[\"model\"][\"training\"][\"batch_size\"],\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(history.history, [],\n",
    "             model_name, img_model.name, optimizer_name, learning_rate,\n",
    "             config[\"data\"][\"cls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model and save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16808, 6) (16808, 576) (16808,)\n"
     ]
    }
   ],
   "source": [
    "test_merged = pd.concat([test.reset_index(drop=True), test_false.reset_index(drop=True)])\n",
    "img_test_merged = np.concatenate([img_test, img_test_false])\n",
    "text_test_merged = np.concatenate([text_test, text_test_false])\n",
    "print(test_merged.shape, img_test_merged.shape, text_test_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16808/16808 [==============================] - 39s 2ms/step - loss: 0.0699\n"
     ]
    }
   ],
   "source": [
    "evaluate(mnn_btl.model, [img_test_merged, text_test_merged], test_merged[[\"label\"]], log_dir, model_name,\n",
    "         img_model.name, optimizer_name, learning_rate, config[\"data\"][\"cls\"], triplet_model=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.8",
   "language": "python",
   "name": "tf2.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
